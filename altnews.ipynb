{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77192667",
   "metadata": {},
   "source": [
    "# Scrapper for Data Acquisition from altnews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f9feb",
   "metadata": {},
   "source": [
    "## https://www.altnews.in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195dccf4",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db28a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\3680683968.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\3680683968.py:27: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  images = driver.find_elements_by_xpath(\"//img\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image: images/image_1.jpg\n",
      "Downloaded image: images/image_2.jpg\n",
      "Downloaded image: images/image_3.jpg\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_2\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the Mastodon explore page\n",
    "driver.get(\"https://www.altnews.in\")\n",
    "\n",
    "# Define the number of posts to scrape\n",
    "num_posts_to_scrape = 3\n",
    "posts_scraped = 0\n",
    "\n",
    "# Create a directory to store downloaded images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Scroll to load more posts\n",
    "while posts_scraped < num_posts_to_scrape:\n",
    "    # Scroll down to load more posts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Find all images on the page\n",
    "    images = driver.find_elements_by_xpath(\"//img\")\n",
    "\n",
    "    # Loop through each image\n",
    "    for image in images:\n",
    "        try:\n",
    "            # Get the image source\n",
    "            image_url = image.get_attribute(\"src\")\n",
    "\n",
    "            # Download the image\n",
    "            if image_url:\n",
    "                image_filename = f\"images/image_{posts_scraped + 1}.jpg\"\n",
    "                with open(image_filename, \"wb\") as f:\n",
    "                    f.write(requests.get(image_url).content)\n",
    "                print(f\"Downloaded image: {image_filename}\")\n",
    "                posts_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {str(e)}\")\n",
    "\n",
    "        if posts_scraped >= num_posts_to_scrape:\n",
    "            break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339105b4",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5a3e9",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f78562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content has been saved to text\\politifact_text.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\3536750187.py:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text_elements = soup.find_all(text=True)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the URL of the website\n",
    "url = \"https://www.altnews.in\"\n",
    "\n",
    "# Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all text elements\n",
    "text_elements = soup.find_all(text=True)\n",
    "\n",
    "# Extract text content and filter out empty strings and whitespaces\n",
    "text_content = [text.strip() for text in text_elements if text.strip()]\n",
    "\n",
    "# Create a folder named \"text\" if it doesn't exist\n",
    "folder_path = \"text\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file within the \"text\" folder\n",
    "csv_file = os.path.join(folder_path, \"politifact_text.csv\")\n",
    "\n",
    "# Write the text content to a CSV file\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file, delimiter=\"\\t\")\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Text\"])\n",
    "\n",
    "    # Write each text content to a separate row\n",
    "    for text in text_content:\n",
    "        writer.writerow([text])\n",
    "\n",
    "print(\"Text content has been saved to\", csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a85a43",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98afb6",
   "metadata": {},
   "source": [
    "## Audio Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f36c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\68954587.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\68954587.py:35: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  audio_elements = driver.find_elements_by_xpath(\"//audio/source\")\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.58)\nStacktrace:\n\tGetHandleVerifier [0x00007FF653D35E42+3538674]\n\t(No symbol) [0x00007FF653954C02]\n\t(No symbol) [0x00007FF653805AEB]\n\t(No symbol) [0x00007FF6537E288C]\n\t(No symbol) [0x00007FF653875DD7]\n\t(No symbol) [0x00007FF65388B40F]\n\t(No symbol) [0x00007FF65386EE53]\n\t(No symbol) [0x00007FF65383F514]\n\t(No symbol) [0x00007FF653840631]\n\tGetHandleVerifier [0x00007FF653D66CAD+3738973]\n\tGetHandleVerifier [0x00007FF653DBC506+4089270]\n\tGetHandleVerifier [0x00007FF653DB4823+4057299]\n\tGetHandleVerifier [0x00007FF653A85C49+720121]\n\t(No symbol) [0x00007FF65396126F]\n\t(No symbol) [0x00007FF65395C304]\n\t(No symbol) [0x00007FF65395C432]\n\t(No symbol) [0x00007FF65394BD04]\n\tBaseThreadInitThunk [0x00007FF9E17E7344+20]\n\tRtlUserThreadStart [0x00007FF9E21226B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Find all audio elements on the page\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m audio_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements_by_xpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//audio/source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Loop through each audio element\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audio_elements:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:549\u001b[0m, in \u001b[0;36mWebDriver.find_elements_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03mFinds multiple elements by xpath.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m        elements = driver.find_elements_by_xpath(\"//div[contains(@class, 'foo')]\")\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    547\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    548\u001b[0m )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_elements(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mXPATH, value\u001b[38;5;241m=\u001b[39mxpath)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:1284\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m-> 1284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m: by,\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: value})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:430\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    428\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    431\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    432\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.58)\nStacktrace:\n\tGetHandleVerifier [0x00007FF653D35E42+3538674]\n\t(No symbol) [0x00007FF653954C02]\n\t(No symbol) [0x00007FF653805AEB]\n\t(No symbol) [0x00007FF6537E288C]\n\t(No symbol) [0x00007FF653875DD7]\n\t(No symbol) [0x00007FF65388B40F]\n\t(No symbol) [0x00007FF65386EE53]\n\t(No symbol) [0x00007FF65383F514]\n\t(No symbol) [0x00007FF653840631]\n\tGetHandleVerifier [0x00007FF653D66CAD+3738973]\n\tGetHandleVerifier [0x00007FF653DBC506+4089270]\n\tGetHandleVerifier [0x00007FF653DB4823+4057299]\n\tGetHandleVerifier [0x00007FF653A85C49+720121]\n\t(No symbol) [0x00007FF65396126F]\n\t(No symbol) [0x00007FF65395C304]\n\t(No symbol) [0x00007FF65395C432]\n\t(No symbol) [0x00007FF65394BD04]\n\tBaseThreadInitThunk [0x00007FF9E17E7344+20]\n\tRtlUserThreadStart [0x00007FF9E21226B1+33]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_2\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the audio page\n",
    "driver.get(\"https://www.altnews.in\")\n",
    "\n",
    "# Define the number of audio files to scrape\n",
    "num_audio_to_scrape = 1\n",
    "audio_scraped = 0\n",
    "\n",
    "# Create an audio directory if it doesn't exist\n",
    "audio_directory = \"audio\"\n",
    "os.makedirs(audio_directory, exist_ok=True)\n",
    "\n",
    "# CSV file path\n",
    "csv_filename = os.path.join(audio_directory, \"audio_links.csv\")\n",
    "csv_exists = os.path.exists(csv_filename)\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    fieldnames = ['Audio URL']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if the file doesn't exist\n",
    "    if not csv_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    # Find all audio elements on the page\n",
    "    audio_elements = driver.find_elements_by_xpath(\"//audio/source\")\n",
    "\n",
    "    # Loop through each audio element\n",
    "    for audio in audio_elements:\n",
    "        try:\n",
    "            # Get the audio source\n",
    "            audio_url = audio.get_attribute(\"src\")\n",
    "\n",
    "            # Check if the URL ends with .mp3, .wav, or .ogg\n",
    "            if audio_url and audio_url.endswith(('.mp3', '.wav', '.ogg')):\n",
    "                # Write audio URL to CSV\n",
    "                writer.writerow({'Audio URL': audio_url})\n",
    "                print(f\"Audio URL added to CSV: {audio_url}\")\n",
    "                audio_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio: {str(e)}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16444f03",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8ba2b",
   "metadata": {},
   "source": [
    "## Video Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9482529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas\\AppData\\Local\\Temp\\ipykernel_7824\\1003981938.py:20: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_driver_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=122.0.6261.58)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF653D35E42+3538674]\n",
      "\t(No symbol) [0x00007FF653954C02]\n",
      "\t(No symbol) [0x00007FF653805AEB]\n",
      "\t(No symbol) [0x00007FF6537E288C]\n",
      "\t(No symbol) [0x00007FF653875DD7]\n",
      "\t(No symbol) [0x00007FF65388B40F]\n",
      "\t(No symbol) [0x00007FF65386EE53]\n",
      "\t(No symbol) [0x00007FF65383F514]\n",
      "\t(No symbol) [0x00007FF653840631]\n",
      "\tGetHandleVerifier [0x00007FF653D66CAD+3738973]\n",
      "\tGetHandleVerifier [0x00007FF653DBC506+4089270]\n",
      "\tGetHandleVerifier [0x00007FF653DB4823+4057299]\n",
      "\tGetHandleVerifier [0x00007FF653A85C49+720121]\n",
      "\t(No symbol) [0x00007FF65396126F]\n",
      "\t(No symbol) [0x00007FF65395C304]\n",
      "\t(No symbol) [0x00007FF65395C432]\n",
      "\t(No symbol) [0x00007FF65394BD04]\n",
      "\tBaseThreadInitThunk [0x00007FF9E17E7344+20]\n",
      "\tRtlUserThreadStart [0x00007FF9E21226B1+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "def scroll_page(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set up the Chrome webdriver\n",
    "        chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_2\\\\chromedriver.exe\"\n",
    "        driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "        # Navigate to the Mastodon explore page\n",
    "        driver.get(\"https://www.altnews.in\")\n",
    "\n",
    "        # Define the number of posts to scrape\n",
    "        num_posts_to_scrape = 1\n",
    "        posts_scraped = 0\n",
    "\n",
    "        # Create a directory to store downloaded video links\n",
    "        videos_folder = \"videos\"\n",
    "        os.makedirs(videos_folder, exist_ok=True)\n",
    "\n",
    "        # Text file to store video links\n",
    "        txt_filename = os.path.join(videos_folder, \"video_links.txt\")\n",
    "\n",
    "        # Scroll to load more posts\n",
    "        while posts_scraped < num_posts_to_scrape:\n",
    "            # Scroll down to load more posts\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # Find all video content within each post\n",
    "            videos = soup.find_all(\"video\")\n",
    "\n",
    "            with open(txt_filename, \"a\", encoding=\"utf-8\") as txt_file:\n",
    "                for video in videos:\n",
    "                    try:\n",
    "                        # Extract video links from the video tag\n",
    "                        video_url = video.get(\"src\")\n",
    "                        if video_url:\n",
    "                            # Write video links to the text file\n",
    "                            txt_file.write(video_url + \"\\n\")\n",
    "                            print(f\"Saved video link: {video_url}\")\n",
    "\n",
    "                            posts_scraped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing video link: {str(e)}\")\n",
    "\n",
    "                    if posts_scraped >= num_posts_to_scrape:\n",
    "                        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fb33e",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aaab87",
   "metadata": {},
   "source": [
    "# Medium Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771184be",
   "metadata": {},
   "source": [
    "### https://medium.com/@waqasdost/scraping-data-from-altnews-website-4db778e5e176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f89",
   "metadata": {},
   "source": [
    "## Citation: Idea and Learing from DataCamp and youtube, Code by ChatGPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0499ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
